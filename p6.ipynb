{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdbe05ed-c6da-4a42-834d-c6e0f3b10452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.25.0.2  104.33 KiB  16      64.2%             bdbda029-3c21-493e-827c-15cb495b7b52  rack1\n",
      "UN  172.25.0.4  104.34 KiB  16      69.7%             cf548ac5-235e-4e43-8336-15d1563d81e9  rack1\n",
      "UN  172.25.0.3  104.32 KiB  16      66.2%             e5d4c00b-49eb-4fb0-b2df-f0042e103afa  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48a488b-1a51-42fe-b8fe-89bf8ec654bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820dc7e2-e566-4d54-8bcf-eb02656ca64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cass.execute(\"drop table if exists stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e81e993-698f-46cf-8cc1-11a60840b2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fdf40248fa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather\")\n",
    "cass.execute(\"\"\"\n",
    "    CREATE KEYSPACE weather\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}\n",
    "\"\"\")\n",
    "\n",
    "cass.set_keyspace(\"weather\")\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TYPE station_record (\n",
    "        tmin INT,\n",
    "        tmax INT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TABLE stations (\n",
    "        id TEXT,\n",
    "        name TEXT STATIC,\n",
    "        date DATE,\n",
    "        record weather.station_record,\n",
    "        PRIMARY KEY ((id), date)\n",
    "    ) WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0ce014-86ef-4dcd-8702-aa5fec139c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE weather.stations (\n",
      "    id text,\n",
      "    date date,\n",
      "    name text static,\n",
      "    record station_record,\n",
      "    PRIMARY KEY (id, date)\n",
      ") WITH CLUSTERING ORDER BY (date ASC)\n",
      "    AND additional_write_policy = '99p'\n",
      "    AND bloom_filter_fp_chance = 0.01\n",
      "    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n",
      "    AND cdc = false\n",
      "    AND comment = ''\n",
      "    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\n",
      "    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n",
      "    AND memtable = 'default'\n",
      "    AND crc_check_chance = 1.0\n",
      "    AND default_time_to_live = 0\n",
      "    AND extensions = {}\n",
      "    AND gc_grace_seconds = 864000\n",
      "    AND max_index_interval = 2048\n",
      "    AND memtable_flush_period_in_ms = 0\n",
      "    AND min_index_interval = 128\n",
      "    AND read_repair = 'BLOCKING'\n",
      "    AND speculative_retry = '99p';\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "print(cass.execute(\"describe table weather.stations\").one().create_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "855f5556-d78e-4a5e-b843-d88af087a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ca961df-fed6-4144-b94c-ce1917a9b5b8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (122ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (299ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (50ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (148ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (36ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (99ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (53ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (29ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (34ms)\n",
      ":: resolution report :: resolve 6163ms :: artifacts dl 1217ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ca961df-fed6-4144-b94c-ce1917a9b5b8\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/92ms)\n",
      "23/11/27 02:58:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5aa4745-4425-406d-afce-d79045788490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "pandas_df1 = df.withColumn(\"ID\", expr(\"substring(value, 0, 11)\"))\n",
    "pandas_df2 = pandas_df1.withColumn(\"STATE\", expr(\"substring(value, 39, 2)\"))\n",
    "pandas_df3 = pandas_df2.withColumn(\"NAME\", expr(\"substring(value, 42, 28)\"))\n",
    "\n",
    "pandas_wisco = pandas_df3.filter(pandas_df3['STATE'] == 'WI').collect()\n",
    "\n",
    "prepared = cass.prepare(\"INSERT INTO weather.stations(id, name) VALUES(?,?)\")\n",
    "\n",
    "for row in pandas_wisco:\n",
    "    station_id = row[\"ID\"]\n",
    "    station_name = row[\"NAME\"]\n",
    "    cass.execute(prepared,(station_id,station_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a70b63-d014-4eb9-8b03-e70843d58292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "name = cass.execute(\"\"\"\n",
    "    select name\n",
    "    from weather.stations\n",
    "    where id = 'USW00014837' \n",
    "\"\"\")\n",
    "\n",
    "for row in name:\n",
    "    station_name = row.name\n",
    "station_name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c243f9b3-d1be-4c05-b1a6-5e7119c34e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "token = cass.execute(\"\"\"\n",
    "    select token(id) as token_id\n",
    "    from weather.stations\n",
    "    where id = 'USC00470273'\n",
    "    \"\"\")\n",
    "\n",
    "for row in token:\n",
    "    token_id = row.token_id\n",
    "    \n",
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb883ef9-43a7-4e29-9835-2c5174fd249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8884495976480270205"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "import subprocess\n",
    "\n",
    "# run nodetool ring command\n",
    "nodetool_output = subprocess.check_output([\"nodetool\", \"ring\"]).decode(\"utf-8\").split('\\n')\n",
    "\n",
    "# token value for USC00470273\n",
    "target_token = \"-9014250178872933741\"\n",
    "\n",
    "token_lines = [line.strip() for line in nodetool_output[5:-6] if line.strip()]\n",
    "# print(token_lines)\n",
    "\n",
    "ring_tokens = [int(line.split()[-1]) for line in token_lines]\n",
    "#print(ring_tokens)\n",
    "\n",
    "next_token = None\n",
    "\n",
    "# check for wrap around\n",
    "if int(target_token) >= max(ring_tokens):\n",
    "    next_token = min(ring_tokens)\n",
    "else:\n",
    "    for token in sorted(ring_tokens):\n",
    "        if token > int(target_token):\n",
    "            next_token = token\n",
    "            break\n",
    "\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aabc377-9b6b-48e6-8d83-25bce645ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('records.zip', 'r') as zip_ref:\n",
    "  zip_ref.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75cd8cb5-bb30-41e4-90fe-64f32f5c7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/27 02:59:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/11/27 02:59:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+------+\n",
      "|    station|      date|  tmin|  tmax|\n",
      "+-----------+----------+------+------+\n",
      "|USW00014898|1970-08-23|-166.0| -71.0|\n",
      "|USW00014839|1970-08-23| 117.0| 194.0|\n",
      "|USW00014839|1970-08-23|  83.0| 150.0|\n",
      "|USW00014839|1970-08-23|  11.0|  83.0|\n",
      "|USW00014839|1970-08-23| 139.0| 261.0|\n",
      "|USR0000WDDG|1970-08-23|-106.0| -39.0|\n",
      "|USR0000WDDG|1970-08-23|-178.0| -56.0|\n",
      "|USW00014837|1970-08-23| -88.0| -38.0|\n",
      "|USR0000WDDG|1970-08-23|-150.0|-106.0|\n",
      "|USW00014839|1970-08-23|   0.0|  39.0|\n",
      "|USW00014839|1970-08-23| 189.0| 222.0|\n",
      "|USW00014839|1970-08-23| 200.0| 294.0|\n",
      "|USW00014837|1970-08-23| 200.0| 322.0|\n",
      "|USW00014898|1970-08-23|-116.0| -60.0|\n",
      "|USW00014839|1970-08-23| 156.0| 233.0|\n",
      "|USR0000WDDG|1970-08-23|-128.0| -61.0|\n",
      "|USR0000WDDG|1970-08-23|-117.0| -33.0|\n",
      "|USR0000WDDG|1970-08-23| -17.0|  50.0|\n",
      "|USW00014898|1970-08-23| 156.0| 256.0|\n",
      "|USW00014837|1970-08-23| 117.0| 256.0|\n",
      "+-----------+----------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"stations\").getOrCreate()\n",
    "\n",
    "parquet_path = 'records.parquet'\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# df.show()\n",
    "\n",
    "pivoted_df = (\n",
    "    df.groupBy(\"station\", \"date\")\n",
    "    .pivot(\"element\")\n",
    "    .agg({\"value\": \"first\"})\n",
    "    .drop(\"null\")\n",
    "    .select(\"station\", \"date\", col(\"TMIN\").alias(\"tmin\"), col(\"TMAX\").alias(\"tmax\"))\n",
    ")\n",
    "\n",
    "# pivoted_df.show()\n",
    "\n",
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "\n",
    "formatted_df = pivoted_df.withColumn(\"date\", from_unixtime(\"date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "with grpc.insecure_channel('localhost:5440') as channel:\n",
    "    stub = station_pb2_grpc.StationStub(channel)\n",
    "    \n",
    "    for row in formatted_df.rdd.collect():\n",
    "        request = station_pb2.RecordTempsRequest(\n",
    "            station=row.station,\n",
    "            date=row.date,\n",
    "            tmin=int(row.tmin),\n",
    "            tmax=int(row.tmax)\n",
    "        )\n",
    "        \n",
    "        # Make gRPC call to insert the measurements\n",
    "        response = stub.RecordTemps(request)\n",
    "        if response.error:\n",
    "            print(f\"Error inserting data for station {row.station} on date {row.date}: {response.error}\")\n",
    "\n",
    "formatted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78135b79-a7a6-4f2d-b643-bea25699453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356.0\n",
      "tmax: 17\n",
      "\n",
      "Error: The calculated max temperature does not match the gRPC response.\n"
     ]
    }
   ],
   "source": [
    "#q5\n",
    "with grpc.insecure_channel('localhost:5440') as channel:\n",
    "    stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "    # find the max temperature for station 'USW00014837'\n",
    "    max_temperature_row = formatted_df.filter(col(\"station\") == \"USW00014837\").select(\"tmax\").agg({\"tmax\": \"max\"}).collect()\n",
    "    max_temperature = max_temperature_row[0][\"max(tmax)\"]\n",
    "    print(max_temperature)\n",
    "    # make gRPC call to get the max temperature for the station\n",
    "    request = station_pb2.StationMaxRequest(station=\"USW00014837\")\n",
    "    response = stub.StationMax(request)\n",
    "\n",
    "    print(response)\n",
    "    \n",
    "    # compare the results\n",
    "    if response.tmax == max_temperature:\n",
    "        print(f\"The max temperature for station USW00014837 is {max_temperature}\")\n",
    "    else:\n",
    "        print(\"Error: The calculated max temperature does not match the gRPC response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c61b9025-ed44-474d-86bc-4b536b45b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"org.apache.spark.sql.cassandra\").option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\").option(\"keyspace\", \"weather\").option(\"table\", \"stations\").load().createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a61fe7f-c12f-4c79-833d-ab3ca1fb2715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f2daeee-3069-4b9e-93bb-da1469a1616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from stations\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d0a97a-3af9-4648-a6bd-c51b399395e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'USW00014839': 89.6986301369863,\n",
       " 'USW00014837': 105.62739726027397,\n",
       " 'USR0000WDDG': 102.06849315068493,\n",
       " 'USW00014898': 102.93698630136986}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/27 03:06:01 WARN ChannelPool: [s0|p6-db-2/172.25.0.4:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=b19bc2a3-e751-42e9-b763-9d68912cf089, APPLICATION_NAME=Spark-Cassandra-Connector-local-1701053904436}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/27 03:06:08 WARN ChannelPool: [s0|p6-db-2/172.25.0.4:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=b19bc2a3-e751-42e9-b763-9d68912cf089, APPLICATION_NAME=Spark-Cassandra-Connector-local-1701053904436}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    }
   ],
   "source": [
    "#q7\n",
    "temperature_diff_df = formatted_df.select(\n",
    "    \"station\",\n",
    "    ((col(\"tmax\") - col(\"tmin\")).alias(\"temp_diff\"))\n",
    ")\n",
    "avg_diff_per_station = temperature_diff_df.groupBy(\"station\").agg({\"temp_diff\": \"avg\"})\n",
    "result_dict = avg_diff_per_station.rdd.collectAsMap()\n",
    "\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a542337-ed4f-4d8d-857c-4059a1fe605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/27 03:06:18 WARN ChannelPool: [s0|p6-db-2/172.25.0.4:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=b19bc2a3-e751-42e9-b763-9d68912cf089, APPLICATION_NAME=Spark-Cassandra-Connector-local-1701053904436}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.25.0.2  86.03 KiB  16      100.0%            bdbda029-3c21-493e-827c-15cb495b7b52  rack1\n",
      "DN  172.25.0.4  86.04 KiB  16      100.0%            cf548ac5-235e-4e43-8336-15d1563d81e9  rack1\n",
      "UN  172.25.0.3  86.01 KiB  16      100.0%            e5d4c00b-49eb-4fb0-b2df-f0042e103afa  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b80f0fd-7275-48af-9aec-3bd66c3821fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need 3 replicas, but only have 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/27 03:09:17 WARN ChannelPool: [s0|p6-db-2/172.25.0.4:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=b19bc2a3-e751-42e9-b763-9d68912cf089, APPLICATION_NAME=Spark-Cassandra-Connector-local-1701053904436}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q9\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "request = station_pb2.StationMaxRequest(station=\"USW00014837\")\n",
    "reply = stub.StationMax(request)\n",
    "print(reply.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "301860b1-1f8d-4501-8516-ff4808563745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.pool:Error attempting to reconnect to 172.25.0.4:9042, scheduling retry in 527.36 seconds: [Errno 113] Tried connecting to [('172.25.0.4', 9042)]. Last error: No route to host\n",
      "23/11/27 03:15:09 WARN ChannelPool: [s0|p6-db-2/172.25.0.4:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=b19bc2a3-e751-42e9-b763-9d68912cf089, APPLICATION_NAME=Spark-Cassandra-Connector-local-1701053904436}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "channel = grpc.insecure_channel('localhost:5440')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "request = station_pb2.RecordTempsRequest(\n",
    "    station=\"US11262023\",\n",
    "    date=\"2023-11-26\",\n",
    "    tmin=1,\n",
    "    tmax=100\n",
    ")\n",
    "reply = stub.RecordTemps(request)\n",
    "reply.error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
